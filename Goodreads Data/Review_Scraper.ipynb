{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41c3c2d",
   "metadata": {},
   "source": [
    "# Goodreads Book Scraper\n",
    "\n",
    "This notebook implements a web scraper for collecting book data from Goodreads, specifically targeting Brandon Sanderson's books. \n",
    "Books to be scraped:\n",
    "- Cosmere â€” Stand-alones\n",
    "  - Elantris\n",
    "  - Warbreaker\n",
    "  - White Sand\n",
    "  - White Sand, Volume 2\n",
    "  - White Sand, Volume 3\n",
    "  - White Sand Omnibus\n",
    "  - Arcanum Unbounded\n",
    "  - The Emperorâ€™s Soul\n",
    "- Mistborn (& Wax & Wayne etc.)\n",
    "  - The Final Empire\n",
    "  - The Well of Ascension\n",
    "  - The Hero of Ages\n",
    "  - Mistborn: Secret History\n",
    "  - The Alloy of Law\n",
    "  - Shadows of Self\n",
    "  - The Bands of Mourning\n",
    "  - The Lost Metal\n",
    "- The Stormlight Archive\n",
    "  - The Way of Kings\n",
    "  - Words of Radiance\n",
    "  - Oathbringer\n",
    "  - Rhythm of War\n",
    "  - Wind and Truth\n",
    "  - Dawnshard\n",
    "  - Edgedancer\n",
    "- The Reckoners\n",
    "  - Steelheart\n",
    "  - Mitosis\n",
    "  - Firefight\n",
    "  - Calamity\n",
    "  - Lux\n",
    "- Skyward Series\n",
    "  - Skyward\n",
    "  - Starsight\n",
    "  - Cytonic\n",
    "  - Sunreach\n",
    "  - Redawn\n",
    "  - Evershore\n",
    "  - Defiant\n",
    "  - Defending Elysium\n",
    "- The Rithmatist\n",
    "- Secret Projects\n",
    "  - Tress of the Emerald Sea\n",
    "  - Yumi and the Nightmare Painter\n",
    "  - The Sunlit Man\n",
    "  - Isles of the Emberdark\n",
    "- Collaborations with Other Authors\n",
    "  - Dark One\n",
    "  - Dark One: Forgotten\n",
    "  - The Original\n",
    "- Alcatraz vs. the Evil Librarians\n",
    "  - Alcatraz vs. the Evil Librarians\n",
    "  - The Scrivenerâ€™s Bones\n",
    "  - The Knights of Crystallia\n",
    "  - The Shattered Lens\n",
    "  - The Dark Talent\n",
    "  - Bastille vs. the Evil Librarians\n",
    "- Other Novellas and Short Stories\n",
    "  - Legion\n",
    "  - Legion: Skin Deep\n",
    "  - Legion: Lies of the Beholder\n",
    "  - Legion: The Many Lives of Stephen Leeds\n",
    "  - Firstborn\n",
    "  - Perfect State\n",
    "  - Snapshot\n",
    "\n",
    "The scraper will collect the following information:\n",
    "\n",
    "- Author\n",
    "- Title\n",
    "- Publication Date\n",
    "- Page count\n",
    "- Genres\n",
    "- Overall rating\n",
    "- Overall reviews\n",
    "- Individual reviews\n",
    "- Individual ratings\n",
    "- Individual review dates\n",
    "- Individual rating likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d49412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import random\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings (for SSL certificate issues)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set more detailed headers to better mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'DNT': '1'  # Do Not Track request header\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "490be9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_details(url):\n",
    "    \"\"\"\n",
    "    Scrape basic book details from Goodreads page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add a small random delay\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        response = requests.get(url, headers=headers, verify=False)  # Added verify=False for SSL\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Print response status and URL for debugging\n",
    "        print(f\"Response status: {response.status_code}\")\n",
    "        print(f\"Final URL: {response.url}\")\n",
    "        \n",
    "        # Save the HTML content for inspection if needed\n",
    "        html_content = response.text\n",
    "        if \"This page isn't available right now\" in html_content:\n",
    "            print(\"Goodreads is blocking our request\")\n",
    "            return None\n",
    "            \n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        \n",
    "        # Initialize book details dictionary\n",
    "        book_details = {}\n",
    "        \n",
    "        # Get title (try multiple possible selectors)\n",
    "        title_element = (soup.find('h1', class_='Text__title1') or \n",
    "                        soup.find('h1', class_='BookPageTitleSection__title') or\n",
    "                        soup.find('h1'))\n",
    "        if title_element:\n",
    "            book_details['title'] = title_element.text.strip()\n",
    "        else:\n",
    "            print(\"Could not find title element\")\n",
    "            \n",
    "        # Get author (try multiple possible selectors)\n",
    "        author_element = (soup.find('span', class_='ContributorLink__name') or\n",
    "                         soup.find('a', class_='ContributorLink') or\n",
    "                         soup.find('span', {'data-testid': 'name'}))\n",
    "        if author_element:\n",
    "            book_details['author'] = author_element.text.strip()\n",
    "        else:\n",
    "            print(\"Could not find author element\")\n",
    "        \n",
    "        # Get publication details\n",
    "        details_div = (soup.find('div', {'data-testid': 'bookDetails'}) or\n",
    "                      soup.find('div', {'data-testid': 'publicationInfo'}))\n",
    "        if details_div:\n",
    "            details_text = details_div.get_text()\n",
    "            \n",
    "            # Extract publication date (try multiple patterns)\n",
    "            pub_date_match = (re.search(r'First published (\\w+ \\d+,? \\d{4})', details_text) or\n",
    "                            re.search(r'Published\\s+(\\w+\\s+\\d+(?:st|nd|rd|th)?,?\\s+\\d{4})', details_text))\n",
    "            book_details['publication_date'] = pub_date_match.group(1) if pub_date_match else None\n",
    "            \n",
    "            # Extract page count\n",
    "            pages_match = re.search(r'(\\d+)\\s*pages?', details_text)\n",
    "            book_details['page_count'] = int(pages_match.group(1)) if pages_match else None\n",
    "        else:\n",
    "            print(\"Could not find publication details\")\n",
    "        \n",
    "        # Get genres (try multiple possible selectors)\n",
    "        genre_elements = (soup.find_all('span', class_='BookPageMetadataSection__genreButton') or\n",
    "                         soup.find_all('span', {'data-testid': 'genreLink'}))\n",
    "        book_details['genres'] = [genre.text.strip() for genre in genre_elements] if genre_elements else []\n",
    "        \n",
    "        # Get overall rating (try multiple possible selectors)\n",
    "        rating_div = (soup.find('div', {'class': 'RatingStatistics__rating'}) or\n",
    "                     soup.find('div', {'data-testid': 'average'}))\n",
    "        if rating_div:\n",
    "            try:\n",
    "                book_details['overall_rating'] = float(rating_div.text.strip())\n",
    "            except ValueError:\n",
    "                print(\"Could not convert rating to float\")\n",
    "                book_details['overall_rating'] = None\n",
    "        else:\n",
    "            print(\"Could not find rating element\")\n",
    "        \n",
    "        # Get review count (try multiple possible selectors)\n",
    "        reviews_element = (soup.find('div', {'data-testid': 'reviewsCount'}) or\n",
    "                          soup.find('span', {'data-testid': 'reviewsCount'}))\n",
    "        if reviews_element:\n",
    "            reviews_text = reviews_element.text.strip()\n",
    "            reviews_count = ''.join(filter(str.isdigit, reviews_text))\n",
    "            book_details['overall_reviews'] = int(reviews_count) if reviews_count else 0\n",
    "        else:\n",
    "            print(\"Could not find reviews count\")\n",
    "            book_details['overall_reviews'] = 0\n",
    "        \n",
    "        # Check if we got any data\n",
    "        if not any(book_details.values()):\n",
    "            print(\"No data was successfully scraped\")\n",
    "            return None\n",
    "            \n",
    "        return book_details\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping book details: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f9132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(url, num_reviews=1000):\n",
    "    \"\"\"\n",
    "    Scrape individual reviews from Goodreads page\n",
    "    Deduplicates reviews as they are collected to ensure unique reviews\n",
    "    \"\"\"\n",
    "    reviews_list = []\n",
    "    seen_review_texts = set()  # Track unique review texts\n",
    "    page = 1\n",
    "    consecutive_no_new_reviews = 0  # Counter for pages with no new reviews\n",
    "    max_pages_without_new = 3  # Stop after 3 pages with no new unique reviews\n",
    "    next_page_token = None\n",
    "    \n",
    "    try:\n",
    "        while len(reviews_list) < num_reviews:\n",
    "            # Construct the reviews page URL with pagination token if available\n",
    "            if page == 1:\n",
    "                reviews_url = f\"{url}/reviews\"\n",
    "            elif next_page_token:\n",
    "                reviews_url = f\"{url}/reviews?reviewFilters={next_page_token}\"\n",
    "            else:\n",
    "                print(\"No more pages available\")\n",
    "                break\n",
    "                \n",
    "            # Add a small random delay\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            response = requests.get(reviews_url, headers=headers, verify=False)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Print debug information\n",
    "            print(f\"Fetching reviews page {page}\")\n",
    "            print(f\"Response status: {response.status_code}\")\n",
    "            print(f\"Final URL: {response.url}\")\n",
    "            \n",
    "            # Check if we're being blocked\n",
    "            if \"This page isn't available right now\" in response.text:\n",
    "                print(\"Goodreads is blocking our request\")\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            # Try different possible review container classes\n",
    "            review_containers = (\n",
    "                soup.find_all('div', class_='ReviewCard') or\n",
    "                soup.find_all('article', class_='ReviewCard') or\n",
    "                soup.find_all('div', class_='Review') or\n",
    "                soup.find_all('div', class_='ReviewsList__review')\n",
    "            )\n",
    "            \n",
    "            print(f\"Found {len(review_containers)} review containers on page {page}\")\n",
    "            \n",
    "            if not review_containers:\n",
    "                print(\"No review containers found. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            new_reviews_this_page = 0\n",
    "            \n",
    "            for container in review_containers:\n",
    "                if len(reviews_list) >= num_reviews:\n",
    "                    break\n",
    "                    \n",
    "                review = {}\n",
    "                \n",
    "                # Get rating (try multiple selectors and fallbacks)\n",
    "                rating_element = None\n",
    "                try:\n",
    "                    # Broad class-based search (case-insensitive, looks for star/rating keywords)\n",
    "                    rating_element = container.find(class_=re.compile(r'(?i)(star|rating|static)'))\n",
    "                except Exception:\n",
    "                    rating_element = None\n",
    "\n",
    "                # Also check for explicit aria-label/title like '5 of 5' (Goodreads sometimes uses this)\n",
    "                if not rating_element:\n",
    "                    rating_element = container.find(attrs={'aria-label': re.compile(r'\\d+\\s+of\\s+5')}) or \\\n",
    "                                     container.find(attrs={'title': re.compile(r'\\d+\\s+of\\s+5')})\n",
    "\n",
    "                review['rating'] = None\n",
    "                if rating_element:\n",
    "                    # Prefer aria-label or title, fall back to text\n",
    "                    rating_text = rating_element.get('aria-label') or rating_element.get('title') or rating_element.text\n",
    "                    rating_match = re.search(r\"(\\d+)\", rating_text)\n",
    "                    review['rating'] = int(rating_match.group(1)) if rating_match else None\n",
    "                \n",
    "                # Get review text (try multiple selectors)\n",
    "                review_text = (\n",
    "                    container.find('div', class_='Formatted') or\n",
    "                    container.find('div', class_='ReviewText') or\n",
    "                    container.find('span', class_='Formatted')\n",
    "                )\n",
    "                review['review_text'] = review_text.text.strip() if review_text else ''\n",
    "                \n",
    "                # Debug: print first 50 chars of each review text on page 2\n",
    "                if page == 2:\n",
    "                    print(f\"  Review text preview: {review['review_text'][:50]}...\")\n",
    "                \n",
    "                # Get review date and reading status (try multiple selectors)\n",
    "                date_element = (\n",
    "                    container.find('span', class_='Text__micro') or\n",
    "                    container.find('span', class_='ReviewDate') or\n",
    "                    container.find('div', class_='ReviewMetadata')\n",
    "                )\n",
    "                if date_element:\n",
    "                    date_text = date_element.text.strip()\n",
    "                    # Extract status if present (e.g., \"currently reading\" or \"finished reading\")\n",
    "                    status_match = re.search(r'(currently reading|finished reading|started reading)', date_text, re.I)\n",
    "                    review['reading_status'] = status_match.group(1).lower() if status_match else 'unknown'\n",
    "                    # Clean date text of status\n",
    "                    clean_date = re.sub(r'(currently reading|finished reading|started reading)', '', date_text, flags=re.I)\n",
    "                    review['review_date'] = clean_date.strip()\n",
    "                \n",
    "                # Enhanced likes extraction with multiple strategies\n",
    "                review['likes'] = 0\n",
    "                \n",
    "                # Strategy 1: Look for elements with like-related text\n",
    "                like_patterns = [\n",
    "                    r'(\\d+)\\s*likes?',\n",
    "                    r'(\\d+)\\s*people liked this',\n",
    "                    r'like this review\\s*\\((\\d+)\\)',\n",
    "                    r'rated it helpful\\s*\\((\\d+)\\)'\n",
    "                ]\n",
    "                \n",
    "                # Look through all text nodes for these patterns\n",
    "                for text in container.stripped_strings:\n",
    "                    for pattern in like_patterns:\n",
    "                        match = re.search(pattern, text, re.I)\n",
    "                        if match:\n",
    "                            review['likes'] = int(match.group(1))\n",
    "                            break\n",
    "                    if review['likes'] > 0:\n",
    "                        break\n",
    "                \n",
    "                # Strategy 2: Look for specific elements if no likes found yet\n",
    "                if review['likes'] == 0:\n",
    "                    # Try finding buttons or spans with like-related attributes\n",
    "                    like_elements = container.find_all(['button', 'span', 'div'], \n",
    "                        attrs={'aria-label': re.compile(r'(\\d+).*like', re.I)})\n",
    "                    \n",
    "                    for elem in like_elements:\n",
    "                        aria_label = elem.get('aria-label', '')\n",
    "                        match = re.search(r'(\\d+)', aria_label)\n",
    "                        if match:\n",
    "                            review['likes'] = int(match.group(1))\n",
    "                            break\n",
    "                \n",
    "                # Strategy 3: Look for elements with specific classes\n",
    "                if review['likes'] == 0:\n",
    "                    like_classes = [\n",
    "                        'likeCount',\n",
    "                        'like-count',\n",
    "                        'likesCount',\n",
    "                        'socialStatistics',\n",
    "                        'social-statistics'\n",
    "                    ]\n",
    "                    for class_name in like_classes:\n",
    "                        elem = container.find(class_=class_name)\n",
    "                        if elem:\n",
    "                            match = re.search(r'(\\d+)', elem.text)\n",
    "                            if match:\n",
    "                                review['likes'] = int(match.group(1))\n",
    "                                break\n",
    "                \n",
    "                # Only append if we got some content AND it's unique\n",
    "                if (review['review_text'] or review['rating']):\n",
    "                    # Create a unique identifier for the review (using review text)\n",
    "                    review_identifier = review['review_text'].strip().lower()\n",
    "                    \n",
    "                    if review_identifier and review_identifier not in seen_review_texts:\n",
    "                        seen_review_texts.add(review_identifier)\n",
    "                        reviews_list.append(review)\n",
    "                        new_reviews_this_page += 1\n",
    "            \n",
    "            print(f\"Added {new_reviews_this_page} new unique reviews from page {page}\")\n",
    "            print(f\"Total unique reviews so far: {len(reviews_list)}\")\n",
    "            \n",
    "            # Extract the next page token from the JSON data embedded in the page\n",
    "            next_page_token = None\n",
    "            \n",
    "            # Look for the __NEXT_DATA__ script tag that contains the JSON\n",
    "            script_tag = soup.find('script', id='__NEXT_DATA__')\n",
    "            if script_tag:\n",
    "                try:\n",
    "                    import json\n",
    "                    import base64\n",
    "                    \n",
    "                    page_data = json.loads(script_tag.string)\n",
    "                    # Navigate through the JSON structure to find nextPageToken\n",
    "                    apollo_state = page_data.get('props', {}).get('pageProps', {}).get('apolloState', {})\n",
    "                    root_query = apollo_state.get('ROOT_QUERY', {})\n",
    "                    reviews_connection = root_query.get('getReviews', {})\n",
    "                    page_info = reviews_connection.get('pageInfo', {})\n",
    "                    raw_token = page_info.get('nextPageToken')\n",
    "                    \n",
    "                    if raw_token:\n",
    "                        # Wrap the token in JSON format and base64 encode it\n",
    "                        token_json = json.dumps({\"after\": raw_token})\n",
    "                        next_page_token = base64.b64encode(token_json.encode('utf-8')).decode('utf-8')\n",
    "                        print(f\"Found and encoded next page token: {next_page_token[:40]}...\")\n",
    "                    else:\n",
    "                        print(\"No nextPageToken found in JSON data\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing JSON for next page token: {e}\")\n",
    "            else:\n",
    "                print(\"Could not find __NEXT_DATA__ script tag\")\n",
    "            \n",
    "            # Check if we're getting new reviews\n",
    "            if new_reviews_this_page == 0:\n",
    "                consecutive_no_new_reviews += 1\n",
    "                print(f\"Warning: No new unique reviews found on this page ({consecutive_no_new_reviews}/{max_pages_without_new})\")\n",
    "                if consecutive_no_new_reviews >= max_pages_without_new:\n",
    "                    print(f\"Stopping: No new unique reviews found after {max_pages_without_new} consecutive pages\")\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_no_new_reviews = 0  # Reset counter\n",
    "            \n",
    "            # Check if there's a next page\n",
    "            if not next_page_token:\n",
    "                print(\"No more pages found (no 'More reviews' button)\")\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "        print(f\"\\nâœ… Successfully scraped {len(reviews_list)} unique reviews\")\n",
    "        return reviews_list\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error while scraping reviews: {e}\")\n",
    "        return reviews_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "        return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c39ce5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris\n",
      "Could not find publication details\n",
      "Book Details:\n",
      "title                                                       Elantris\n",
      "author                                             Brandon Sanderson\n",
      "genres             [Fantasy, Fiction, Audiobook, High Fantasy, Ep...\n",
      "overall_rating                                                  4.17\n",
      "overall_reviews                                                25282\n",
      "dtype: object\n",
      "\n",
      "Fetching reviews from: https://www.goodreads.com/book/show/68427.Elantris\n",
      "Fetching reviews page 1\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews\n",
      "Found 30 review containers on page 1\n",
      "Added 30 new unique reviews from page 1\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Fetching reviews page 1\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews\n",
      "Found 30 review containers on page 1\n",
      "Added 30 new unique reviews from page 1\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Fetching reviews page 2\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews?reviewFilters=eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJek1UQTAifQ==\n",
      "Found 30 review containers on page 2\n",
      "  Review text preview: To be fair, I was warned going in that this was Sa...\n",
      "  Review text preview: 3.5/5 StarsExactly 3 months ago, I finished binge ...\n",
      "  Review text preview: 4.31!â€œRemember, the past need not become our futur...\n",
      "  Review text preview: Oh my God this is so contrary to my usual love of ...\n",
      "  Review text preview: Oh, Elantris, why must you torture me so? Why must...\n",
      "  Review text preview: I'll have to think about how to review this but fo...\n",
      "  Review text preview: when i asked john for book recommendations, his li...\n",
      "  Review text preview: full five stars for part two <3...\n",
      "  Review text preview: \"Iâ€™ve only been a duke for ten years.\"\n",
      "If it hadnâ€™...\n",
      "  Review text preview: Holy shite balls! I loved this book!Okay, I took a...\n",
      "  Review text preview: Re-discovering Brandon Sanderson was one of my bes...\n",
      "  Review text preview: I confess myself disappointed with Mr Sanderson. F...\n",
      "  Review text preview: 3.75â­ï¸I got my Cosmere fix. *giddy. Giddy. So gidd...\n",
      "  Review text preview: 3,5/5TrochÄ™ zbyt rozwleczona, ale uwielbiam gÅ‚Ã³wnÄ…...\n",
      "  Review text preview: What happens after the Fall?Sanderson sets up a co...\n",
      "  Review text preview: Even if this is â€œbaby\" Sanderson, as the author hi...\n",
      "  Review text preview: Warning: the review that follows is terribly unpro...\n",
      "  Review text preview: 4'5 / 5Creo que contiene spoilers, asÃ­ que si no h...\n",
      "  Review text preview: If I were rating  Elantris purely on the world bui...\n",
      "  Review text preview: Call me an Elantrian cuz I'll never recover....\n",
      "  Review text preview: Una buena introducciÃ³n al universo creado por Sand...\n",
      "  Review text preview: between 2 and 3 stars... i need to think it throug...\n",
      "  Review text preview: 5/5Sin ninguna duda, por lo que he disfrutado con ...\n",
      "  Review text preview: I DID IT! Took me only about 4 months but i did it...\n",
      "  Review text preview: My first Brandon Sanderson novel and I can say I l...\n",
      "  Review text preview: This was SO GOOD ðŸ¤ Rtc <33...\n",
      "  Review text preview: Review number 2Having just finished re-reading the...\n",
      "  Review text preview: Rated 4.5/5 stars!\n",
      "What do I say besides I really ...\n",
      "  Review text preview: I read the so-called â€œworstâ€ cosmere book by Brand...\n",
      "  Review text preview: *I finished this two months ago and am still think...\n",
      "Added 0 new unique reviews from page 2\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Warning: No new unique reviews found on this page (1/3)\n",
      "Fetching reviews page 2\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews?reviewFilters=eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJek1UQTAifQ==\n",
      "Found 30 review containers on page 2\n",
      "  Review text preview: To be fair, I was warned going in that this was Sa...\n",
      "  Review text preview: 3.5/5 StarsExactly 3 months ago, I finished binge ...\n",
      "  Review text preview: 4.31!â€œRemember, the past need not become our futur...\n",
      "  Review text preview: Oh my God this is so contrary to my usual love of ...\n",
      "  Review text preview: Oh, Elantris, why must you torture me so? Why must...\n",
      "  Review text preview: I'll have to think about how to review this but fo...\n",
      "  Review text preview: when i asked john for book recommendations, his li...\n",
      "  Review text preview: full five stars for part two <3...\n",
      "  Review text preview: \"Iâ€™ve only been a duke for ten years.\"\n",
      "If it hadnâ€™...\n",
      "  Review text preview: Holy shite balls! I loved this book!Okay, I took a...\n",
      "  Review text preview: Re-discovering Brandon Sanderson was one of my bes...\n",
      "  Review text preview: I confess myself disappointed with Mr Sanderson. F...\n",
      "  Review text preview: 3.75â­ï¸I got my Cosmere fix. *giddy. Giddy. So gidd...\n",
      "  Review text preview: 3,5/5TrochÄ™ zbyt rozwleczona, ale uwielbiam gÅ‚Ã³wnÄ…...\n",
      "  Review text preview: What happens after the Fall?Sanderson sets up a co...\n",
      "  Review text preview: Even if this is â€œbaby\" Sanderson, as the author hi...\n",
      "  Review text preview: Warning: the review that follows is terribly unpro...\n",
      "  Review text preview: 4'5 / 5Creo que contiene spoilers, asÃ­ que si no h...\n",
      "  Review text preview: If I were rating  Elantris purely on the world bui...\n",
      "  Review text preview: Call me an Elantrian cuz I'll never recover....\n",
      "  Review text preview: Una buena introducciÃ³n al universo creado por Sand...\n",
      "  Review text preview: between 2 and 3 stars... i need to think it throug...\n",
      "  Review text preview: 5/5Sin ninguna duda, por lo que he disfrutado con ...\n",
      "  Review text preview: I DID IT! Took me only about 4 months but i did it...\n",
      "  Review text preview: My first Brandon Sanderson novel and I can say I l...\n",
      "  Review text preview: This was SO GOOD ðŸ¤ Rtc <33...\n",
      "  Review text preview: Review number 2Having just finished re-reading the...\n",
      "  Review text preview: Rated 4.5/5 stars!\n",
      "What do I say besides I really ...\n",
      "  Review text preview: I read the so-called â€œworstâ€ cosmere book by Brand...\n",
      "  Review text preview: *I finished this two months ago and am still think...\n",
      "Added 0 new unique reviews from page 2\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Warning: No new unique reviews found on this page (1/3)\n",
      "Fetching reviews page 3\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews?reviewFilters=eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJek1UQTAifQ==\n",
      "Found 30 review containers on page 3\n",
      "Added 0 new unique reviews from page 3\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Warning: No new unique reviews found on this page (2/3)\n",
      "Fetching reviews page 3\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews?reviewFilters=eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJek1UQTAifQ==\n",
      "Found 30 review containers on page 3\n",
      "Added 0 new unique reviews from page 3\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Warning: No new unique reviews found on this page (2/3)\n",
      "Fetching reviews page 4\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews?reviewFilters=eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJek1UQTAifQ==\n",
      "Found 30 review containers on page 4\n",
      "Added 0 new unique reviews from page 4\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Warning: No new unique reviews found on this page (3/3)\n",
      "Stopping: No new unique reviews found after 3 consecutive pages\n",
      "\n",
      "âœ… Successfully scraped 30 unique reviews\n",
      "\n",
      "Reviews Summary:\n",
      "Total reviews fetched: 30\n",
      "\n",
      "Columns present: ['rating', 'review_text', 'likes', 'review_length', 'book_title', 'word_count']\n",
      "\n",
      "First few rows:\n",
      "   rating                                        review_text  likes  \\\n",
      "0     5.0  To be fair, I was warned going in that this wa...    149   \n",
      "1     3.0  3.5/5 StarsExactly 3 months ago, I finished bi...    474   \n",
      "2     4.0  4.31!â€œRemember, the past need not become our f...    348   \n",
      "3     4.0  Oh my God this is so contrary to my usual love...    293   \n",
      "4     2.0  Oh, Elantris, why must you torture me so? Why ...    277   \n",
      "\n",
      "   review_length book_title  word_count  \n",
      "0            907   Elantris         180  \n",
      "1           4376   Elantris         739  \n",
      "2           3338   Elantris         624  \n",
      "3           1987   Elantris         356  \n",
      "4           3450   Elantris         563  \n",
      "\n",
      "Data types:\n",
      "rating           float64\n",
      "review_text       object\n",
      "likes              int64\n",
      "review_length      int64\n",
      "book_title        object\n",
      "word_count         int64\n",
      "dtype: object\n",
      "Fetching reviews page 4\n",
      "Response status: 200\n",
      "Final URL: https://www.goodreads.com/book/show/68427.Elantris/reviews?reviewFilters=eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJek1UQTAifQ==\n",
      "Found 30 review containers on page 4\n",
      "Added 0 new unique reviews from page 4\n",
      "Total unique reviews so far: 30\n",
      "Found and encoded next page token: eyJhZnRlciI6ICJPREF6Tml3eE5UUTJOek0xTXpJ...\n",
      "Warning: No new unique reviews found on this page (3/3)\n",
      "Stopping: No new unique reviews found after 3 consecutive pages\n",
      "\n",
      "âœ… Successfully scraped 30 unique reviews\n",
      "\n",
      "Reviews Summary:\n",
      "Total reviews fetched: 30\n",
      "\n",
      "Columns present: ['rating', 'review_text', 'likes', 'review_length', 'book_title', 'word_count']\n",
      "\n",
      "First few rows:\n",
      "   rating                                        review_text  likes  \\\n",
      "0     5.0  To be fair, I was warned going in that this wa...    149   \n",
      "1     3.0  3.5/5 StarsExactly 3 months ago, I finished bi...    474   \n",
      "2     4.0  4.31!â€œRemember, the past need not become our f...    348   \n",
      "3     4.0  Oh my God this is so contrary to my usual love...    293   \n",
      "4     2.0  Oh, Elantris, why must you torture me so? Why ...    277   \n",
      "\n",
      "   review_length book_title  word_count  \n",
      "0            907   Elantris         180  \n",
      "1           4376   Elantris         739  \n",
      "2           3338   Elantris         624  \n",
      "3           1987   Elantris         356  \n",
      "4           3450   Elantris         563  \n",
      "\n",
      "Data types:\n",
      "rating           float64\n",
      "review_text       object\n",
      "likes              int64\n",
      "review_length      int64\n",
      "book_title        object\n",
      "word_count         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# URLs for books by Brandon Sanderson\n",
    "'''\n",
    "urls = [\"https://www.goodreads.com/book/show/68427.Elantris\",\n",
    "\"https://www.goodreads.com/book/show/1268479.Warbreaker\",\n",
    "\"https://www.goodreads.com/book/show/28862254-white-sand-volume-1\",\n",
    "\"https://www.goodreads.com/book/show/33551363-white-sand-volume-2\",\n",
    "\"https://www.goodreads.com/book/show/39298848-white-sand-volume-3\",\n",
    "\"https://www.goodreads.com/book/show/60696519-white-sand-omnibus\",\n",
    "\"https://www.goodreads.com/book/show/28595941-arcanum-unbounded\",\n",
    "\"https://www.goodreads.com/book/show/13578175-the-emperor-s-soul\",\n",
    "\"https://www.goodreads.com/book/show/68428.Mistborn\",\n",
    "\"https://www.goodreads.com/book/show/68429.The_Well_of_Ascension\",\n",
    "\"https://www.goodreads.com/book/show/2767793-the-hero-of-ages\",\n",
    "\"https://www.goodreads.com/book/show/28698036-secret-history\",\n",
    "\"https://www.goodreads.com/book/show/10803121-the-alloy-of-law\",\n",
    "\"https://www.goodreads.com/book/show/16065004-shadows-of-self\",\n",
    "\"https://www.goodreads.com/book/show/18739426-the-bands-of-mourning\",\n",
    "\"https://www.goodreads.com/book/show/23947089-the-lost-metal\",\n",
    "\"https://www.goodreads.com/book/show/7235533-the-way-of-kings\",\n",
    "\"https://www.goodreads.com/book/show/17332218-words-of-radiance\",\n",
    "\"https://www.goodreads.com/book/show/34703445-edgedancer\",\n",
    "\"https://www.goodreads.com/book/show/34002132-oathbringer\",\n",
    "\"https://www.goodreads.com/book/show/54511226-dawnshard\",\n",
    "\"https://www.goodreads.com/book/show/49021976-rhythm-of-war\",\n",
    "\"https://www.goodreads.com/book/show/203578847-wind-and-truth\",\n",
    "\"https://www.goodreads.com/book/show/17182126-steelheart\",\n",
    "\"https://www.goodreads.com/book/show/18966322-mitosis\",\n",
    "\"https://www.goodreads.com/book/show/15704459-firefight\",\n",
    "\"https://www.goodreads.com/book/show/15704486-calamity\",\n",
    "\"https://www.goodreads.com/book/show/58419574-lux\",\n",
    "\"https://www.goodreads.com/book/show/13552643-defending-elysium\",\n",
    "\"https://www.goodreads.com/book/show/36642458-skyward\",\n",
    "\"https://www.goodreads.com/book/show/42769202-starsight\",\n",
    "\"https://www.goodreads.com/book/show/57903876-sunreach\",\n",
    "\"https://www.goodreads.com/book/show/57903879-redawn\",\n",
    "\"https://www.goodreads.com/book/show/58465495-evershore\",\n",
    "\"https://www.goodreads.com/book/show/57571215-cytonic\",\n",
    "\"https://www.goodreads.com/book/show/43606308-defiant\",\n",
    "\"https://www.goodreads.com/book/show/60531406-tress-of-the-emerald-sea\",\n",
    "\"https://www.goodreads.com/book/show/60531410-the-frugal-wizard-s-handbook-for-surviving-medieval-england\",\n",
    "\"https://www.goodreads.com/book/show/60531416-yumi-and-the-nightmare-painter\",\n",
    "\"https://www.goodreads.com/book/show/60531420-the-sunlit-man\",\n",
    "\"https://www.goodreads.com/book/show/210300489-isles-of-the-emberdark\",\n",
    "\"https://www.goodreads.com/book/show/49798827-dark-one\",\n",
    "\"https://www.goodreads.com/book/show/60373696-dark-one\",\n",
    "\"https://www.goodreads.com/book/show/54615879-the-original\",\n",
    "\"https://www.goodreads.com/series/45320-alcatraz-vs-the-evil-librarians\",\n",
    "\"https://www.goodreads.com/book/show/3485562-alcatraz-versus-the-scrivener-s-bones\",\n",
    "\"https://www.goodreads.com/book/show/6366110-alcatraz-versus-the-knights-of-crystallia\",\n",
    "\"https://www.goodreads.com/book/show/7740659-alcatraz-versus-the-shattered-lens\",\n",
    "\"https://www.goodreads.com/book/show/26114421-the-dark-talent\",\n",
    "\"https://www.goodreads.com/book/show/59808314-bastille-vs-the-evil-librarians\",\n",
    "\"https://www.goodreads.com/book/show/13452375-legion\",\n",
    "\"https://www.goodreads.com/book/show/20886354-skin-deep\",\n",
    "\"https://www.goodreads.com/book/show/37640636-lies-of-the-beholder\",\n",
    "\"https://www.goodreads.com/book/show/39332065-legion\",\n",
    "\"https://www.goodreads.com/book/show/8562526-firstborn\",\n",
    "\"https://www.goodreads.com/book/show/25188109-perfect-state\",\n",
    "\"https://www.goodreads.com/book/show/31176804-snapshot\"\n",
    "]\n",
    "\n",
    "# Loop through each URL in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing: {url}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Get book details\n",
    "        book_details = get_book_details(url)\n",
    "        if not book_details:\n",
    "            print(f\"Skipping {url} - could not fetch book details\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nBook: {book_details.get('title', 'Unknown')}\")\n",
    "\n",
    "        # Fetch reviews for each book\n",
    "        print(f\"\\nFetching reviews...\")\n",
    "        reviews = get_reviews(url, num_reviews=1000)\n",
    "        \n",
    "        if not reviews:\n",
    "            print(f\"No reviews found for {book_details.get('title', 'Unknown')}. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        reviews_df = pd.DataFrame(reviews)\n",
    "\n",
    "        # Add book_title, review_length, and word_count columns\n",
    "        reviews_df['book_title'] = book_details.get('title', 'Unknown')\n",
    "        reviews_df['review_length'] = reviews_df['review_text'].apply(lambda x: len(str(x)))\n",
    "        reviews_df['word_count'] = reviews_df['review_text'].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        # Reorder columns to match desired format\n",
    "        reviews_df = reviews_df[['rating', 'review_text', 'likes', 'review_length', 'book_title', 'word_count']]\n",
    "        \n",
    "        book_df = pd.DataFrame([book_details])\n",
    "\n",
    "        # Create filenames dynamically\n",
    "        safe_title = re.sub(r'[^a-zA-Z0-9_-]', '_', book_details.get('title', 'unknown'))\n",
    "        book_df.to_csv(f'{safe_title}_details.csv', index=False)\n",
    "        reviews_df.to_csv(f'{safe_title}_reviews.csv', index=False)\n",
    "\n",
    "        print(f\"\\nâœ… Saved {safe_title} data successfully ({len(reviews_df)} reviews)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {url}: {e}\")\n",
    "        continue\n",
    "'''\n",
    "\n",
    "# Use this to test with a single book\n",
    "url = \"https://www.goodreads.com/book/show/68427.Elantris\"\n",
    "\n",
    "# Get book details\n",
    "book_details = get_book_details(url)\n",
    "\n",
    "# Check if book_details was successfully retrieved\n",
    "if book_details:\n",
    "    print(\"Book Details:\")\n",
    "    print(pd.Series(book_details))\n",
    "\n",
    "    # Get reviews\n",
    "    print(\"\\nFetching reviews from:\", url)\n",
    "    reviews = get_reviews(url, num_reviews=1000)\n",
    "\n",
    "    # Convert reviews to DataFrame\n",
    "    reviews_df = pd.DataFrame(reviews)\n",
    "\n",
    "    # Add book_title, review_length, and word_count columns\n",
    "    reviews_df['book_title'] = book_details.get('title', 'Unknown')\n",
    "    reviews_df['review_length'] = reviews_df['review_text'].apply(lambda x: len(str(x)))\n",
    "    reviews_df['word_count'] = reviews_df['review_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # Reorder columns to match desired format: rating, review_text, likes, review_length, book_title, word_count\n",
    "    reviews_df = reviews_df[['rating', 'review_text', 'likes', 'review_length', 'book_title', 'word_count']]\n",
    "\n",
    "    # Show summary stats\n",
    "    print(\"\\nReviews Summary:\")\n",
    "    print(f\"Total reviews fetched: {len(reviews_df)}\")\n",
    "    print(\"\\nColumns present:\", list(reviews_df.columns))\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(reviews_df.head())\n",
    "    print(\"\\nData types:\")\n",
    "    print(reviews_df.dtypes)\n",
    "else:\n",
    "    print(\"Failed to retrieve book details. Cannot proceed with review scraping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e684ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to CSV files\n",
    "book_df = pd.DataFrame([book_details])\n",
    "book_df.to_csv('elantris_details.csv', index=False)\n",
    "reviews_df.to_csv('elantris_reviews.csv', index=False)\n",
    "\n",
    "print(\"\\nData has been saved to 'elantris_details.csv' and 'elantris_reviews.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
