{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2aae23",
   "metadata": {},
   "source": [
    "# Goodreads Book Scraper - Selenium Version\n",
    "\n",
    "This notebook uses Selenium to scrape Goodreads reviews by simulating browser interactions.\n",
    "This allows us to click the \"Show more reviews\" button and load additional pages dynamically.\n",
    "\n",
    "## Books to be scraped:\n",
    "Brandon Sanderson's 57 books (same list as the original scraper)\n",
    "\n",
    "## Data collected:\n",
    "- Book details: Author, Title, Publication Date, Page count, Genres, Overall rating, Overall reviews\n",
    "- Review details: rating, review_text, likes, review_length, book_title, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102200c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Set up and return a configured Chrome WebDriver\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # Run in background (remove this to see the browser)\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36')\n",
    "    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a855f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_details_selenium(driver, url):\n",
    "    \"\"\"\n",
    "    Scrape basic book details from Goodreads page using Selenium\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching book details from: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2, 4))  # Wait for page to load\n",
    "        \n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        # Initialize book details dictionary\n",
    "        book_details = {}\n",
    "        \n",
    "        # Get title\n",
    "        title_element = (soup.find('h1', class_='Text__title1') or \n",
    "                        soup.find('h1', class_='BookPageTitleSection__title') or\n",
    "                        soup.find('h1'))\n",
    "        book_details['title'] = title_element.text.strip() if title_element else 'Unknown'\n",
    "        \n",
    "        # Get author\n",
    "        author_element = (soup.find('span', class_='ContributorLink__name') or\n",
    "                         soup.find('a', class_='ContributorLink') or\n",
    "                         soup.find('span', {'data-testid': 'name'}))\n",
    "        book_details['author'] = author_element.text.strip() if author_element else 'Unknown'\n",
    "        \n",
    "        # Get publication details\n",
    "        details_div = (soup.find('div', {'data-testid': 'bookDetails'}) or\n",
    "                      soup.find('div', {'data-testid': 'publicationInfo'}))\n",
    "        if details_div:\n",
    "            details_text = details_div.get_text()\n",
    "            \n",
    "            # Extract publication date\n",
    "            pub_date_match = (re.search(r'First published (\\w+ \\d+,? \\d{4})', details_text) or\n",
    "                            re.search(r'Published\\s+(\\w+\\s+\\d+(?:st|nd|rd|th)?,?\\s+\\d{4})', details_text))\n",
    "            book_details['publication_date'] = pub_date_match.group(1) if pub_date_match else None\n",
    "            \n",
    "            # Extract page count\n",
    "            pages_match = re.search(r'(\\d+)\\s*pages?', details_text)\n",
    "            book_details['page_count'] = int(pages_match.group(1)) if pages_match else None\n",
    "        else:\n",
    "            book_details['publication_date'] = None\n",
    "            book_details['page_count'] = None\n",
    "        \n",
    "        # Get genres\n",
    "        genre_elements = (soup.find_all('span', class_='BookPageMetadataSection__genreButton') or\n",
    "                         soup.find_all('span', {'data-testid': 'genreLink'}))\n",
    "        book_details['genres'] = [genre.text.strip() for genre in genre_elements] if genre_elements else []\n",
    "        \n",
    "        # Get overall rating\n",
    "        rating_div = (soup.find('div', {'class': 'RatingStatistics__rating'}) or\n",
    "                     soup.find('div', {'data-testid': 'average'}))\n",
    "        if rating_div:\n",
    "            try:\n",
    "                book_details['overall_rating'] = float(rating_div.text.strip())\n",
    "            except ValueError:\n",
    "                book_details['overall_rating'] = None\n",
    "        else:\n",
    "            book_details['overall_rating'] = None\n",
    "        \n",
    "        # Get review count\n",
    "        reviews_element = (soup.find('div', {'data-testid': 'reviewsCount'}) or\n",
    "                          soup.find('span', {'data-testid': 'reviewsCount'}))\n",
    "        if reviews_element:\n",
    "            reviews_text = reviews_element.text.strip()\n",
    "            reviews_count = ''.join(filter(str.isdigit, reviews_text))\n",
    "            book_details['overall_reviews'] = int(reviews_count) if reviews_count else 0\n",
    "        else:\n",
    "            book_details['overall_reviews'] = 0\n",
    "        \n",
    "        print(f\"✓ Successfully scraped details for: {book_details['title']}\")\n",
    "        return book_details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error scraping book details: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ecedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_selenium(driver, url, num_reviews=1000, max_clicks=50):\n",
    "    \"\"\"\n",
    "    Scrape reviews from Goodreads using Selenium to click \"Show more reviews\" button\n",
    "    \n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance\n",
    "    - url: Book URL\n",
    "    - num_reviews: Target number of unique reviews to collect\n",
    "    - max_clicks: Maximum number of times to click \"Show more\" button\n",
    "    \"\"\"\n",
    "    reviews_list = []\n",
    "    seen_review_texts = set()\n",
    "    clicks = 0\n",
    "    consecutive_no_new = 0\n",
    "    max_consecutive_no_new = 3\n",
    "    \n",
    "    try:\n",
    "        # Navigate to reviews page\n",
    "        reviews_url = f\"{url}/reviews\"\n",
    "        print(f\"\\nNavigating to: {reviews_url}\")\n",
    "        driver.get(reviews_url)\n",
    "        time.sleep(random.uniform(3, 5))  # Wait for initial page load\n",
    "        \n",
    "        while len(reviews_list) < num_reviews and clicks < max_clicks:\n",
    "            # Parse current page content\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            \n",
    "            # Find all review containers\n",
    "            review_containers = (\n",
    "                soup.find_all('div', class_='ReviewCard') or\n",
    "                soup.find_all('article', class_='ReviewCard') or\n",
    "                soup.find_all('div', class_='Review')\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nIteration {clicks + 1}: Found {len(review_containers)} review containers\")\n",
    "            new_reviews_this_iteration = 0\n",
    "            \n",
    "            # Extract review data\n",
    "            for container in review_containers:\n",
    "                if len(reviews_list) >= num_reviews:\n",
    "                    break\n",
    "                \n",
    "                review = {}\n",
    "                \n",
    "                # Get rating\n",
    "                rating_element = container.find(class_=re.compile(r'(?i)(star|rating|static)'))\n",
    "                if not rating_element:\n",
    "                    rating_element = container.find(attrs={'aria-label': re.compile(r'\\d+\\s+of\\s+5')})\n",
    "                \n",
    "                if rating_element:\n",
    "                    rating_text = rating_element.get('aria-label') or rating_element.get('title') or rating_element.text\n",
    "                    rating_match = re.search(r\"(\\d+)\", rating_text)\n",
    "                    review['rating'] = int(rating_match.group(1)) if rating_match else None\n",
    "                else:\n",
    "                    review['rating'] = None\n",
    "                \n",
    "                # Get review text\n",
    "                review_text_elem = (\n",
    "                    container.find('div', class_='Formatted') or\n",
    "                    container.find('div', class_='ReviewText') or\n",
    "                    container.find('span', class_='Formatted')\n",
    "                )\n",
    "                review['review_text'] = review_text_elem.text.strip() if review_text_elem else ''\n",
    "                \n",
    "                # Get likes\n",
    "                review['likes'] = 0\n",
    "                like_patterns = [\n",
    "                    r'(\\d+)\\s*likes?',\n",
    "                    r'(\\d+)\\s*people liked this'\n",
    "                ]\n",
    "                \n",
    "                for text in container.stripped_strings:\n",
    "                    for pattern in like_patterns:\n",
    "                        match = re.search(pattern, text, re.I)\n",
    "                        if match:\n",
    "                            review['likes'] = int(match.group(1))\n",
    "                            break\n",
    "                    if review['likes'] > 0:\n",
    "                        break\n",
    "                \n",
    "                # Check if review is unique and has content\n",
    "                if review['review_text']:\n",
    "                    review_identifier = review['review_text'].strip().lower()\n",
    "                    \n",
    "                    if review_identifier and review_identifier not in seen_review_texts:\n",
    "                        seen_review_texts.add(review_identifier)\n",
    "                        reviews_list.append(review)\n",
    "                        new_reviews_this_iteration += 1\n",
    "            \n",
    "            print(f\"  → Added {new_reviews_this_iteration} new unique reviews\")\n",
    "            print(f\"  → Total unique reviews: {len(reviews_list)}/{num_reviews}\")\n",
    "            \n",
    "            # Check if we got new reviews\n",
    "            if new_reviews_this_iteration == 0:\n",
    "                consecutive_no_new += 1\n",
    "                print(f\"  ⚠ No new reviews found ({consecutive_no_new}/{max_consecutive_no_new})\")\n",
    "                \n",
    "                if consecutive_no_new >= max_consecutive_no_new:\n",
    "                    print(f\"\\n⛔ Stopping: No new reviews after {max_consecutive_no_new} attempts\")\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_no_new = 0\n",
    "            \n",
    "            # Check if we have enough reviews\n",
    "            if len(reviews_list) >= num_reviews:\n",
    "                print(f\"\\n✓ Target reached: {len(reviews_list)} unique reviews collected\")\n",
    "                break\n",
    "            \n",
    "            # Try to click \"Show more reviews\" button\n",
    "            try:\n",
    "                # Wait for the button to be present and clickable\n",
    "                wait = WebDriverWait(driver, 5)\n",
    "                \n",
    "                # Try multiple selectors for the \"Show more\" button\n",
    "                button = None\n",
    "                button_selectors = [\n",
    "                    (By.XPATH, \"//button[contains(., 'Show more reviews')]\"),\n",
    "                    (By.XPATH, \"//button[contains(@data-testid, 'loadMore')]\"),\n",
    "                    (By.XPATH, \"//button[contains(., 'more reviews')]\"),\n",
    "                    (By.CSS_SELECTOR, \"button[data-testid='loadMore']\")\n",
    "                ]\n",
    "                \n",
    "                for by, selector in button_selectors:\n",
    "                    try:\n",
    "                        button = wait.until(EC.element_to_be_clickable((by, selector)))\n",
    "                        break\n",
    "                    except TimeoutException:\n",
    "                        continue\n",
    "                \n",
    "                if button:\n",
    "                    # Scroll to button\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # Click the button\n",
    "                    try:\n",
    "                        button.click()\n",
    "                    except ElementClickInterceptedException:\n",
    "                        # Try JavaScript click if regular click fails\n",
    "                        driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    \n",
    "                    clicks += 1\n",
    "                    print(f\"  ✓ Clicked 'Show more' button (click #{clicks})\")\n",
    "                    \n",
    "                    # Wait for new content to load\n",
    "                    time.sleep(random.uniform(2, 4))\n",
    "                else:\n",
    "                    print(\"\\n⛔ No 'Show more' button found - reached end of reviews\")\n",
    "                    break\n",
    "                    \n",
    "            except TimeoutException:\n",
    "                print(\"\\n⛔ Timeout waiting for 'Show more' button - no more reviews available\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n⚠ Error clicking button: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✓ Successfully scraped {len(reviews_list)} unique reviews\")\n",
    "        print(f\"  Total button clicks: {clicks}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return reviews_list\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error scraping reviews: {e}\")\n",
    "        return reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204351a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single book first\n",
    "url = \"https://www.goodreads.com/book/show/68427.Elantris\"\n",
    "\n",
    "# Initialize the driver\n",
    "print(\"Initializing Chrome WebDriver...\")\n",
    "driver = setup_driver()\n",
    "\n",
    "try:\n",
    "    # Get book details\n",
    "    book_details = get_book_details_selenium(driver, url)\n",
    "    \n",
    "    if book_details:\n",
    "        print(\"\\nBook Details:\")\n",
    "        print(pd.Series(book_details))\n",
    "        \n",
    "        # Get reviews (try to get 100 for testing, then you can increase to 1000)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Starting review scraping...\")\n",
    "        print(\"=\"*60)\n",
    "        reviews = get_reviews_selenium(driver, url, num_reviews=100, max_clicks=10)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        reviews_df = pd.DataFrame(reviews)\n",
    "        \n",
    "        # Add additional columns\n",
    "        reviews_df['book_title'] = book_details.get('title', 'Unknown')\n",
    "        reviews_df['review_length'] = reviews_df['review_text'].apply(lambda x: len(str(x)))\n",
    "        reviews_df['word_count'] = reviews_df['review_text'].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        # Reorder columns\n",
    "        reviews_df = reviews_df[['rating', 'review_text', 'likes', 'review_length', 'book_title', 'word_count']]\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total reviews collected: {len(reviews_df)}\")\n",
    "        print(f\"\\nColumns: {list(reviews_df.columns)}\")\n",
    "        print(f\"\\nFirst 5 reviews:\")\n",
    "        print(reviews_df.head())\n",
    "        print(f\"\\nData types:\")\n",
    "        print(reviews_df.dtypes)\n",
    "    else:\n",
    "        print(\"Failed to retrieve book details.\")\n",
    "\n",
    "finally:\n",
    "    # Always close the driver\n",
    "    print(\"\\nClosing browser...\")\n",
    "    driver.quit()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41face8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test results\n",
    "if 'reviews_df' in locals() and not reviews_df.empty:\n",
    "    book_df = pd.DataFrame([book_details])\n",
    "    book_df.to_csv('elantris_details_selenium.csv', index=False)\n",
    "    reviews_df.to_csv('elantris_reviews_selenium.csv', index=False)\n",
    "    print(\"✓ Data saved to CSV files\")\n",
    "else:\n",
    "    print(\"No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967b167",
   "metadata": {},
   "source": [
    "## Full Scraper - All 57 Books\n",
    "\n",
    "Once you've tested the scraper above and confirmed it works, run this cell to scrape all books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cdfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for all Brandon Sanderson books\n",
    "urls = [\n",
    "    \"https://www.goodreads.com/book/show/68427.Elantris\",\n",
    "    \"https://www.goodreads.com/book/show/1268479.Warbreaker\",\n",
    "    \"https://www.goodreads.com/book/show/28862254-white-sand-volume-1\",\n",
    "    \"https://www.goodreads.com/book/show/33551363-white-sand-volume-2\",\n",
    "    \"https://www.goodreads.com/book/show/39298848-white-sand-volume-3\",\n",
    "    \"https://www.goodreads.com/book/show/60696519-white-sand-omnibus\",\n",
    "    \"https://www.goodreads.com/book/show/28595941-arcanum-unbounded\",\n",
    "    \"https://www.goodreads.com/book/show/13578175-the-emperor-s-soul\",\n",
    "    \"https://www.goodreads.com/book/show/68428.Mistborn\",\n",
    "    \"https://www.goodreads.com/book/show/68429.The_Well_of_Ascension\",\n",
    "    \"https://www.goodreads.com/book/show/2767793-the-hero-of-ages\",\n",
    "    \"https://www.goodreads.com/book/show/28698036-secret-history\",\n",
    "    \"https://www.goodreads.com/book/show/10803121-the-alloy-of-law\",\n",
    "    \"https://www.goodreads.com/book/show/16065004-shadows-of-self\",\n",
    "    \"https://www.goodreads.com/book/show/18739426-the-bands-of-mourning\",\n",
    "    \"https://www.goodreads.com/book/show/23947089-the-lost-metal\",\n",
    "    \"https://www.goodreads.com/book/show/7235533-the-way-of-kings\",\n",
    "    \"https://www.goodreads.com/book/show/17332218-words-of-radiance\",\n",
    "    \"https://www.goodreads.com/book/show/34703445-edgedancer\",\n",
    "    \"https://www.goodreads.com/book/show/34002132-oathbringer\",\n",
    "    \"https://www.goodreads.com/book/show/54511226-dawnshard\",\n",
    "    \"https://www.goodreads.com/book/show/49021976-rhythm-of-war\",\n",
    "    \"https://www.goodreads.com/book/show/203578847-wind-and-truth\",\n",
    "    \"https://www.goodreads.com/book/show/17182126-steelheart\",\n",
    "    \"https://www.goodreads.com/book/show/18966322-mitosis\",\n",
    "    \"https://www.goodreads.com/book/show/15704459-firefight\",\n",
    "    \"https://www.goodreads.com/book/show/15704486-calamity\",\n",
    "    \"https://www.goodreads.com/book/show/58419574-lux\",\n",
    "    \"https://www.goodreads.com/book/show/13552643-defending-elysium\",\n",
    "    \"https://www.goodreads.com/book/show/36642458-skyward\",\n",
    "    \"https://www.goodreads.com/book/show/42769202-starsight\",\n",
    "    \"https://www.goodreads.com/book/show/57903876-sunreach\",\n",
    "    \"https://www.goodreads.com/book/show/57903879-redawn\",\n",
    "    \"https://www.goodreads.com/book/show/58465495-evershore\",\n",
    "    \"https://www.goodreads.com/book/show/57571215-cytonic\",\n",
    "    \"https://www.goodreads.com/book/show/43606308-defiant\",\n",
    "    \"https://www.goodreads.com/book/show/60531406-tress-of-the-emerald-sea\",\n",
    "    \"https://www.goodreads.com/book/show/60531410-the-frugal-wizard-s-handbook-for-surviving-medieval-england\",\n",
    "    \"https://www.goodreads.com/book/show/60531416-yumi-and-the-nightmare-painter\",\n",
    "    \"https://www.goodreads.com/book/show/60531420-the-sunlit-man\",\n",
    "    \"https://www.goodreads.com/book/show/210300489-isles-of-the-emberdark\",\n",
    "    \"https://www.goodreads.com/book/show/49798827-dark-one\",\n",
    "    \"https://www.goodreads.com/book/show/60373696-dark-one\",\n",
    "    \"https://www.goodreads.com/book/show/54615879-the-original\",\n",
    "    \"https://www.goodreads.com/book/show/40590407-alcatraz-vs-the-evil-librarians\",\n",
    "    \"https://www.goodreads.com/book/show/3485562-alcatraz-versus-the-scrivener-s-bones\",\n",
    "    \"https://www.goodreads.com/book/show/6366110-alcatraz-versus-the-knights-of-crystallia\",\n",
    "    \"https://www.goodreads.com/book/show/7740659-alcatraz-versus-the-shattered-lens\",\n",
    "    \"https://www.goodreads.com/book/show/26114421-the-dark-talent\",\n",
    "    \"https://www.goodreads.com/book/show/59808314-bastille-vs-the-evil-librarians\",\n",
    "    \"https://www.goodreads.com/book/show/13452375-legion\",\n",
    "    \"https://www.goodreads.com/book/show/20886354-skin-deep\",\n",
    "    \"https://www.goodreads.com/book/show/37640636-lies-of-the-beholder\",\n",
    "    \"https://www.goodreads.com/book/show/39332065-legion\",\n",
    "    \"https://www.goodreads.com/book/show/8562526-firstborn\",\n",
    "    \"https://www.goodreads.com/book/show/25188109-perfect-state\",\n",
    "    \"https://www.goodreads.com/book/show/31176804-snapshot\"\n",
    "]\n",
    "\n",
    "# Initialize the driver\n",
    "print(\"Initializing Chrome WebDriver...\")\n",
    "driver = setup_driver()\n",
    "\n",
    "all_books_data = []\n",
    "all_reviews_data = []\n",
    "failed_urls = []\n",
    "\n",
    "try:\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing Book {i}/{len(urls)}\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Get book details\n",
    "            book_details = get_book_details_selenium(driver, url)\n",
    "            \n",
    "            if not book_details:\n",
    "                print(f\"⚠ Skipping {url} - could not fetch book details\")\n",
    "                failed_urls.append(url)\n",
    "                continue\n",
    "            \n",
    "            # Get reviews\n",
    "            reviews = get_reviews_selenium(driver, url, num_reviews=1000, max_clicks=50)\n",
    "            \n",
    "            if not reviews:\n",
    "                print(f\"⚠ No reviews found for {book_details.get('title', 'Unknown')}\")\n",
    "                failed_urls.append(url)\n",
    "                continue\n",
    "            \n",
    "            # Convert reviews to DataFrame\n",
    "            reviews_df = pd.DataFrame(reviews)\n",
    "            reviews_df['book_title'] = book_details.get('title', 'Unknown')\n",
    "            reviews_df['review_length'] = reviews_df['review_text'].apply(lambda x: len(str(x)))\n",
    "            reviews_df['word_count'] = reviews_df['review_text'].apply(lambda x: len(str(x).split()))\n",
    "            reviews_df = reviews_df[['rating', 'review_text', 'likes', 'review_length', 'book_title', 'word_count']]\n",
    "            \n",
    "            # Save individual book files\n",
    "            safe_title = re.sub(r'[^a-zA-Z0-9_-]', '_', book_details.get('title', 'unknown'))\n",
    "            book_df = pd.DataFrame([book_details])\n",
    "            book_df.to_csv(f'{safe_title}_details.csv', index=False)\n",
    "            reviews_df.to_csv(f'{safe_title}_reviews.csv', index=False)\n",
    "            \n",
    "            # Add to combined lists\n",
    "            all_books_data.append(book_details)\n",
    "            all_reviews_data.append(reviews_df)\n",
    "            \n",
    "            print(f\"\\n✓ Successfully saved {safe_title} ({len(reviews_df)} reviews)\")\n",
    "            \n",
    "            # Be polite - wait between books\n",
    "            if i < len(urls):\n",
    "                wait_time = random.uniform(5, 10)\n",
    "                print(f\"\\n⏳ Waiting {wait_time:.1f} seconds before next book...\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing {url}: {e}\")\n",
    "            failed_urls.append(url)\n",
    "            continue\n",
    "    \n",
    "    # Save combined files\n",
    "    if all_books_data:\n",
    "        combined_books = pd.DataFrame(all_books_data)\n",
    "        combined_books.to_csv('Combined_Details_Selenium.csv', index=False)\n",
    "        print(f\"\\n✓ Saved combined book details ({len(combined_books)} books)\")\n",
    "    \n",
    "    if all_reviews_data:\n",
    "        combined_reviews = pd.concat(all_reviews_data, ignore_index=True)\n",
    "        combined_reviews.to_csv('Combined_Reviews_Selenium.csv', index=False)\n",
    "        print(f\"✓ Saved combined reviews ({len(combined_reviews)} reviews)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total books processed: {len(all_books_data)}/{len(urls)}\")\n",
    "    print(f\"Total reviews collected: {sum(len(df) for df in all_reviews_data)}\")\n",
    "    print(f\"Failed URLs: {len(failed_urls)}\")\n",
    "    if failed_urls:\n",
    "        print(\"\\nFailed URLs:\")\n",
    "        for url in failed_urls:\n",
    "            print(f\"  - {url}\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\n\\nClosing browser...\")\n",
    "    driver.quit()\n",
    "    print(\"✓ Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
